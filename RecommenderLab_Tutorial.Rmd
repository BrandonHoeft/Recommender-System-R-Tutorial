---
title: "RecommenderLab Tutorial"
author: "Brandon Hoeft"
date: "October 6, 2017"
output: 
  html_document: 
    keep_md: yes # http://happygitwithr.com/rmd-test-drive.html#output-format
    toc: yes
    toc_depth: 3
---

```{r setup, include = FALSE}
# my global options defined for each code chunk.
knitr::opts_chunk$set(fig.width=7, fig.height=5, echo=FALSE, warning=FALSE, message=FALSE, comment = '')
```

## Introduction

This is an introduction to building Recommender Systems using R. The major CRAN approved package available in R with developed algorithms is called `recommenderlab` by Michael Hahsler. Latest [documentation](https://cran.r-project.org/web/packages/recommenderlab/recommenderlab.pdf) and a [vignette](https://cran.r-project.org/web/packages/recommenderlab/vignettes/recommenderlab.pdf) are both available for exploration. The code examples provided in this exploratory analysis came primarily through the material on Collaborative Filtering algorithms from this package, explored in the book [*Building a Recommendation System with R*](https://smile.amazon.com/Building-Recommendation-System-Suresh-Gorakala/dp/1783554495/ref=sr_1_1?ie=UTF8&qid=1507314554&sr=8-1&keywords=building+a+recommendation+system+R), by Suresh K. Gorakala and Michele Usuelli. 

## Collaborative Filtering

The focus of this analysis will center around [*collaborative filtering*](https://en.wikipedia.org/wiki/Collaborative_filtering), one of the earliest forms of recommendation systems. The earliest developed forms of these algorithms are also known as *neighborhood based* or *memory based* algorithms, described below. If using machine learning or statistical model methods, they're referred to as *model based* algorithms. The basic idea of collaborative filtering is that given a large database of ratings profiles for individual users on what they rated/purchased, we can impute or predict ratings on items not rated/purchased by them, forming the basis of recommendation scores or top-N recommended items. 

Under *user-based collaborative filtering*, this memory-based method works under the assumption that users with similar item tastes will rate items similarly. Therefore, the missing ratings for a user can be predicted by finding other similar users (a neighborhood). Within the neighborhood, we can aggregate the ratings of these neighbors on items unknown to the user, as basis for a prediction.

An inverted approach to nearest neighbor based recommendations is *item-based collaborative filtering*. Instead of finding the most similar users to each individual, an algorithm assesses the similarities between the items that are correlated in their ratings or purchase profile amongst all users. 

Some additional starter articles to learning more about collaborative filtering can be found [here](https://www.ibm.com/developerworks/library/os-recommender1/) and here(http://recommender-systems.org/collaborative-filtering/). 

## Load recommenderlab

Let's load the package and explore some of the datasets included in it. Recommenderlab is implemented using classes in the **S4** class system, so it's notation is a little different from most `r` packages, which are often written using the **S3** object class system. 

``` {r}
library(dplyr)
library(ggplot2)
library(recommenderlab)
```

Some of the preloaded datasets that come with `recommenderlab` for learning and exploring. 

``` {r}
help(package = "recommenderlab")
datasets_available <- data(package = "recommenderlab")
datasets_available$results[,4] # titles
```

We'll work with the already available *Movielense* dataset.

``` {r}
data(MovieLense) # loads dataset
class(MovieLense)
movie_r <- MovieLense 
remove(MovieLense)
```

It is formatted as a `realRatingMatrix` class already, an object class created within `recommenderlab` for efficient storage of user-item ratings matrices. It's been optimized for storing sparse matrices, where almost all of the elements are empty. As an example, compare the object size of *Movielense* as a `realRatingMatrix` vs. a `matrix`. 

``` {r}
library(pryr)
object_size(movie_r)
object_size(as(movie_r, "matrix"))
```

The `realRatingMatrix` for this particular dataset is about 9 times more efficient in conserving memory than a traditional matrix object.

Some of the different functions that can be applied to the `realRatingMatrix` are: 

``` {r}
methods(class = "realRatingMatrix")
```

## Exploratory Analysis of the Movielense data

Some initial information about the dimensions and ratings count within Movielense matrix. 

``` {r}
movie_r
```

A preview of the first 25 users (rows of matrix) shows their count of movie ratings out of the 1664 available movies in the dataset. 

``` {r }
rowCounts(movie_r[1:25,])
```

Below is a preview of the ratings matrix of users and their ratings. Rows represent the user indexes.

``` {r }
getRatingMatrix(movie_r[1:10, 1:5])
```


Let's preview some of the movies rated by User #1. User 1's given an average rating of `r round(rowMeans(movie_r[1, ]), 2)`.

``` {r}
as(movie_r[1, ], "list")[[1]][1:10]
```

The `getRatings` function returns the non-missing ratings values from the matrix as a numeric vector. The following histogram shows the distribution of all movie ratings in the dataset. We can see that ratings typically skew higher, centered around a median rating of 4. 

``` {r echo = TRUE}
summary(getRatings(movie_r))

data.frame(ratings = getRatings(movie_r)) %>%
  ggplot(aes(ratings)) + geom_bar(width = 0.75) +
    labs(title = 'Movielense Ratings Distribution')

```

Using our `realRatingMatrix` object, we can also extract row counts to visualize distributions of the number of reviews given by each user. Below, the density is plotted along the y-axis instead of the raw counts, to give an idea of the the proportional frequency of each unit of each discrete bin in relation to the whole data set. The overall right-skewed distribution is indicative that most reviewers give very few overall reviews. 

In terms of understanding the density values, this histogram has bin-width set to 20; with a density of close to 0.01125 for the first bin, the tallest bar this bin represents approximately 0.01125 x 10 units per bin = `r 0.01125 * 20` total proportion of the individual reviewers in the data. In other words, `r 0.01125 * 20 * 100`% of the `r nrow(movie_r)` in the data have given fewer than 10 reviews. 

``` {r echo = TRUE}
summary(rowCounts(movie_r))

rowCounts(movie_r) %>%
  data.frame(reviews_per_person = .) %>%
  ggplot(aes(x = reviews_per_person)) + 
    geom_histogram(aes(y = ..density..), binwidth = 20) +
    scale_y_continuous(limits = c(0,.0125), 
                       breaks = seq(0, .0125, by = 0.0025),
                       labels = seq(0, .0125, by = 0.0025)) +
    labs(title = 'Number of Ratings Per MovieLense Reviewer')

```

Additionally, we can take a look at the average number of ratings given per each of the `r ncol(movie_r)` movies. Again, the right-skewed distribution here is indicative that the majority of films in the dataset are scarcely reviewed and there are a handful of movies with very high reviews, probably reflecting those films in the dataset with mass commercial appeal. 

With a median number of reviews of `r median(colCounts(movie_r))` per user and `r ncol(movie_r)` different movies available to rate, we know that the data is sparse with a lot of users not having rated most of the movies available. 

``` {r echo = TRUE}
summary(colCounts(movie_r))

colCounts(movie_r) %>%
  data.frame(movie_review_count = .) %>%
  ggplot(aes(x = movie_review_count)) + 
    geom_histogram(aes(y = ..density..), binwidth = 20) +
    scale_y_continuous(limits = c(0,.0175)) +
    labs(title = 'Number of Reviews Per MovieLense listed Movie')

```

**Can also visually explore summary(rowMeans(movie_r)) for average rating given per user.**

**Can also visually explore summary(colMeans(movie_r)) for average rating given per movie.**

## Recommender Algorithms Available

The recommender algorithms are stored in a registry object called `recommenderRegistry`. We can get a look at the different models based on the different matrix types.

``` {r echo = TRUE}
names(recommenderRegistry$get_entries())
```

Since our matrix is a real ratings matrix, we'll call the algorithms available for working on numeric ratings based review data as stored in the `realRatingMatrix`. Here, I've pulled the descriptions of each of the algorithms available for working with real user ratings data. 

``` {r eval = FALSE, echo = TRUE}
vapply(recommenderRegistry$get_entries(dataType = "realRatingMatrix"), 
       '[[', 
       FUN.VALUE = "character", 
       "description")
```

## Exploring User-based Collaborative Filtering

In the algorithms registry, the last algorithm provided in the listing is the one we'll use to explore user-based collaborative filtering (UBCF) to fit the UBCF algorithm to the `realRatingMatrix` of MovieLense reviews data. Information about this algorithm per the registry:

``` {r}
tail(recommenderRegistry$get_entries(dataType = "realRatingMatrix"), 1)
```

There are 4 parameters to account for with this model as described above:

* **method**: this is the type of similarity metric to calculate similarity between users real ratings profile. Cosine similarity, Pearson correlation coefficient, and Jaccard similarity are available options. The first two are not good options if using unary ratings, but work well for this scenario.

* **nn**: this parameter sets the neighborhood of most similar users to consider for each user profile. the ratings profiles of the k nearest neighbors will be the basis for making predictions on a users unrated items profile.

* **sample**: a logical value to indicate whether the data should be sampled for train/test. Probably best to explicitely set a reproducible seed and sample the data before running the model.

* **normalize**: how to normalize real ratings provided by different users. This is crucially important b/c all users have a different bias in how they tend to rate items. This can be done by passing a value to this parameter inside the algorithm or applied to the matrix before any modeling too. See `?normalize` for additional details. 

### Normalize the data 

Since we're working with explicit real ratings of users, we need to acocunt for individual row bias of each user and make sure that all ratings are scaled similarly. The implication of not doing this could be potentially disasterous on new predicted ratings for any given user, dependent upon the different ratings bias of their k nearest neighbors. 

Here, user rating *zero mean centering* is used, where each user's vector of ratings is subtracted by its own mean to center the mean at zero. Z-scoring is an alternative method available too that additionally divides each user's rating by its standard deviation. 

** maybe visualize the distribution of user ratings here too after normalization vs. before normalization **
``` {r}

movie_r_norm <- normalize(movie_r, method = 'center')

```

### Split the data into Train & Test

So we can more objectively measure the performance of the UBCF recommender system, we will build it on a subset of training records, and test the model on a different subset of testing records that were withheld from the modeling process. 

``` {r}
set.seed(123)
train_index <- sample(c(TRUE, FALSE), size = nrow(movie_r_norm), 
                      replace = TRUE, 
                      prob = c (.7, .3)) # weights for train/test size

train <- movie_r_norm[train_index, ]
test <- movie_r_norm[!train_index, ]

# check that train and test records sum up to movie_r records count. 
sum(nrow(train), nrow(test)) == nrow(movie_r_norm)
```

### Fit the model on Training Data

Having zero mean centered each user's ratings to control for user bias, and splitting the data into model training and model testing partitions, time to fit a model. Parameters were tuned from defaults for illustrative purposes. 

``` {r echo = TRUE}
ubcf_model <- Recommender(train, method = "UBCF", 
                          parameter = list(method = "cosine",
                                           nn = 10, # find each user's 10 most similar users. 
                                           sample = FALSE, # already did this.
                                           normalize = NA)) # already did this. 
```

Now that we have a model, we can look at its metadata, such as the hyperparameters used to filter user recommendations. 

``` {r echo = TRUE}
getModel(ubcf_model)
```



## Strengs & Weaknesses of Neighborhood Methods

* *Data Requirements*: a user ratings profile, containing items theyâ€™ve rated/clicked/purchased. A "rating" can be defined however it fits the business use case.
        	
* *Strengths*: simple to implement, and recommendations are easy to explain to user. Transparency about the recommendation to a user can be a great boost to the user's confidence in trusting a rating. 
 
* *Weaknesses*: these algorithms do not too work well on very sparse ratings matrices. Additionally, they are computationally expensive as the entire user database needs to be processed as the basis of forming recommendations. These algorithms will not work from a cold start since a new user has no historic data profile or ratings for the algorithm to start from. 
