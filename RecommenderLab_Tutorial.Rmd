---
title: "RecommenderLab Tutorial"
author: "Brandon Hoeft"
date: "October 6, 2017"
output: 
  html_document: 
    keep_md: yes # http://happygitwithr.com/rmd-test-drive.html#output-format
    toc: yes
    toc_depth: 3
---

```{r setup, include = FALSE}
# my global options defined for each code chunk.
knitr::opts_chunk$set(fig.width=7, fig.height=5, echo=FALSE, warning=FALSE, message=FALSE, comment = '')
```

## Introduction

This is an introduction to building Recommender Systems using R. The major CRAN approved package available in R with developed algorithms is called `recommenderlab` by Michael Hahsler. Latest [documentation](https://cran.r-project.org/web/packages/recommenderlab/recommenderlab.pdf) and a [vignette](https://cran.r-project.org/web/packages/recommenderlab/vignettes/recommenderlab.pdf) are both available for exploration. The code examples provided in this exploratory analysis came primarily through the material on Collaborative Filtering algorithms from this package, explored in the book [*Building a Recommendation System with R*](https://smile.amazon.com/Building-Recommendation-System-Suresh-Gorakala/dp/1783554495/ref=sr_1_1?ie=UTF8&qid=1507314554&sr=8-1&keywords=building+a+recommendation+system+R), by Suresh K. Gorakala and Michele Usuelli. 

## Collaborative Filtering

The focus of this analysis will center around [*collaborative filtering*](https://en.wikipedia.org/wiki/Collaborative_filtering), one of the earliest forms of recommendation systems. The earliest developed forms of these algorithms are also known as *neighborhood based* or *memory based* algorithms, described below. If using machine learning or statistical model methods, they're referred to as *model based* algorithms. The basic idea of collaborative filtering is that given a large database of ratings profiles for individual users on what they rated/purchased, we can impute or predict ratings on items not rated/purchased by them, forming the basis of recommendation scores or top-N recommended items. 

Under *user-based collaborative filtering*, this memory-based method works under the assumption that users with similar item tastes will rate items similarly. Therefore, the missing ratings for a user can be predicted by finding other similar users (a neighborhood). Within the neighborhood, we can aggregate the ratings of these neighbors on items unknown to the user, as basis for a prediction.

An inverted approach to nearest neighbor based recommendations is *item-based collaborative filtering*. Instead of finding the most similar users to each individual, an algorithm assesses the similarities between the items that are correlated in their ratings or purchase profile amongst all users. 

Some additional starter articles to learning more about collaborative filtering can be found [here](https://www.ibm.com/developerworks/library/os-recommender1/) and here(http://recommender-systems.org/collaborative-filtering/). 

## Load recommenderlab

Let's load the package and explore some of the datasets included in it. Recommenderlab is implemented using classes in the **S4** class system, so it's notation is a little different from most `r` packages, which are often written using the **S3** object class system. 

``` {r}
library(dplyr)
library(ggplot2)
library(recommenderlab)
```

Some of the preloaded datasets that come with `recommenderlab` for learning and exploring. 

``` {r}
help(package = "recommenderlab")
datasets_available <- data(package = "recommenderlab")
datasets_available$results[,4] # titles
```

We'll work with the already available *Movielense* dataset.

``` {r}
data(MovieLense) # loads dataset
class(MovieLense)
movie_r <- MovieLense 
remove(MovieLense)
```

It is formatted as a `realRatingMatrix` class already, an object class created within `recommenderlab` for efficient storage of user-item ratings matrices. It's been optimized for storing sparse matrices, where almost all of the elements are empty. As an example, compare the object size of *Movielense* as a `realRatingMatrix` vs. a `matrix`. 

``` {r}
library(pryr)
object_size(movie_r)
object_size(as(movie_r, "matrix"))
```

The `realRatingMatrix` for this particular dataset is about 9 times more efficient in conserving memory than a traditional matrix object.

Some of the different functions that can be applied to the `realRatingMatrix` are: 

``` {r}
methods(class = "realRatingMatrix")
```

## Exploratory Analysis of the Movielense data

Some initial information about the dimensions and ratings count within Movielense matrix. 

``` {r}
movie_r
```

A preview of the first 25 users (rows of matrix) shows their count of movie ratings out of the 1664 available movies in the dataset. 

``` {r }
rowCounts(movie_r[1:25,])
```

Let's preview some of the movies rated by User #1. User 1's given an average rating of `r round(rowMeans(movie_r[1, ]), 2)`.

``` {r}
as(movie_r[1, ], "list")[[1]][1:10]
```

The `getRatings` function returns the non-missing ratings values from the matrix as a numeric vector. The following histogram shows the distribution of the extracted movie ratings. We can see that ratings typically skew higher, centered around a median rating of 4. 

``` {r echo = TRUE}
summary(getRatings(movie_r))

data.frame(ratings = getRatings(movie_r)) %>%
  ggplot(aes(ratings)) + geom_bar(width = 0.75) +
    labs(title = 'Movielense Ratings Distribution')

```

Using our `realRatingMatrix` object, we can also extract row counts to visualize distributions of the number of reviews given by each user. Below, the density is plotted along the y-axis instead of the raw counts, to give an idea of the the proportional frequency of each unit of each discrete bin in relation to the whole data set. The overall right-skewed distribution is indicative that most reviewers give very few overall reviews. 

In terms of understanding the density values, this histogram has bin-width set to 20; with a density of close to 0.01125 for the first bin, the tallest bar this bin represents approximately 0.01125 x 10 units per bin = `r 0.01125 * 20` total proportion of the individual reviewers in the data. In other words, `r 0.01125 * 20 * 100`% of the `r nrow(movie_r)` in the data have given fewer than 10 reviews. 

``` {r echo = TRUE}
summary(rowCounts(movie_r))

rowCounts(movie_r) %>%
  data.frame(reviews_per_person = .) %>%
  ggplot(aes(x = reviews_per_person)) + 
    geom_histogram(aes(y = ..density..), binwidth = 20) +
    scale_y_continuous(limits = c(0,.0125), 
                       breaks = seq(0, .0125, by = 0.0025),
                       labels = seq(0, .0125, by = 0.0025)) +
    labs(title = 'Number of Ratings Per MovieLense Reviewer')

```

Additionally, we can take a look at the average number of ratings given per each of the `r ncol(movie_r)` movies. Again, the right-skewed distribution here is indicative that the majority of films in the dataset are scarcely reviewed and there are a handful of movies with very high reviews, probably reflecting those films in the dataset with mass commercial appeal. 

``` {r echo = TRUE}
summary(colCounts(movie_r))

colCounts(movie_r) %>%
  data.frame(movie_review_count = .) %>%
  ggplot(aes(x = movie_review_count)) + 
    geom_histogram(aes(y = ..density..), binwidth = 20) +
    scale_y_continuous(limits = c(0,.0175)) +
    labs(title = 'Number of Reviews Per MovieLense listed Movie')

```

**Can also visually explore summary(rowMeans(movie_r)) for average rating given per user.**

**Can also visually explore summary(colMeans(movie_r)) for average rating given per movie.**

## Recommender Algorithms Available

The recommender algorithms are stored in a registry object called `recommenderRegistry`. We can call the algorithms available for working on numeric ratings based review data. 

``` {r echo = TRUE}
recommenderRegistry$get_entries(dataType = "realRatingMatrix")
```

## Exploring User-based Collaborative Filtering

In the registry above, the last algorithm provided in the listing is the one we'll use to explore user-based collaborative filtering (UBCF). We fit the UBCF algorithm to our `realRatingMatrix` of MovieLense reviews data below. The API is very straightforward, and the model can be generated in a single line of code when using the default hyper parameters.

``` {r echo = TRUE}
ubcf_model <- Recommender(movie_r, method = "UBCF")
```

Now that we have a model, we can look at its metadata, such as the hyperparameters used to filter user recommendations. 

``` {r echo = TRUE}
getModel(ubcf_model)
```



## Strengs & Weaknesses of Neighborhood Methods

* *Data Requirements*: a user ratings profile, containing items theyâ€™ve rated/clicked/purchased. A "rating" can be defined however it fits the business use case.
        	
* *Strengths*: simple to implement, and recommendations are easy to explain to user. Transparency about the recommendation to a user can be a great boost to the user's confidence in trusting a rating. 
 
* *Weaknesses*: these algorithms do not too work well on very sparse ratings matrices. Additionally, they are computationally expensive as the entire user database needs to be processed as the basis of forming recommendations. These algorithms will not work from a cold start since a new user has no historic data profile or ratings for the algorithm to start from. 
