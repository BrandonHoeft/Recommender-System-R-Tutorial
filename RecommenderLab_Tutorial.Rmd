---
title: "RecommenderLab Tutorial"
author: "Brandon Hoeft"
date: "October 6, 2017"
output: 
  html_document: 
    keep_md: yes # http://happygitwithr.com/rmd-test-drive.html#output-format
    toc: yes
    toc_depth: 3
---

```{r setup, include = FALSE}
# my global options defined for each code chunk.
knitr::opts_chunk$set(fig.width=7, fig.height=5, echo=FALSE, warning=FALSE, message=FALSE, comment = '')
```

## Introduction

This is an introduction to building Recommender Systems using R. The major CRAN approved package available in R with developed algorithms is called `recommenderlab` by Michael Hahsler. Latest [documentation](https://cran.r-project.org/web/packages/recommenderlab/recommenderlab.pdf) and a [vignette](https://cran.r-project.org/web/packages/recommenderlab/vignettes/recommenderlab.pdf) are both available for exploration. The code examples provided in this exploratory analysis came primarily through the material on Collaborative Filtering algorithms from this package, explored in the book [*Building a Recommendation System with R*](https://smile.amazon.com/Building-Recommendation-System-Suresh-Gorakala/dp/1783554495/ref=sr_1_1?ie=UTF8&qid=1507314554&sr=8-1&keywords=building+a+recommendation+system+R), by Suresh K. Gorakala and Michele Usuelli. 

## Collaborative Filtering

The focus of this analysis will center around [*collaborative filtering*](https://en.wikipedia.org/wiki/Collaborative_filtering), one of the earliest forms of recommendation systems. The earliest developed forms of these algorithms are also known as *neighborhood based* or *memory based* algorithms, described below. If using machine learning or statistical model methods, they're referred to as *model based* algorithms. The basic idea of collaborative filtering is that given a large database of ratings profiles for individual users on what they rated/purchased, we can impute or predict ratings on items not rated/purchased by them, forming the basis of recommendation scores or top-N recommended items. 

Under *user-based collaborative filtering*, this memory-based method works under the assumption that users with similar item tastes will rate items similarly. Therefore, the missing ratings for a user can be predicted by finding other similar users (a neighborhood). Within the neighborhood, we can aggregate the ratings of these neighbors on items unknown to the user, as basis for a prediction. We'll explore this one in detail in sections below.

An inverted approach to nearest neighbor based recommendations is *item-based collaborative filtering*. Instead of finding the most similar users to each individual, an algorithm assesses the similarities between the items that are correlated in their ratings or purchase profile amongst all users. 

Some additional starter articles to learning more about collaborative filtering can be found [here](https://www.ibm.com/developerworks/library/os-recommender1/) and here(http://recommender-systems.org/collaborative-filtering/). 

## Load recommenderlab

Let's load the package and explore some of the datasets included in it. Recommenderlab is implemented using classes in the **S4** class system, so it's notation is a little different from most `r` packages, which are often written using the **S3** object class system. 

``` {r}
library(dplyr)
library(ggplot2)
library(recommenderlab)
```

Some of the preloaded datasets that come with `recommenderlab` for learning and exploring. 

``` {r}
help(package = "recommenderlab")
datasets_available <- data(package = "recommenderlab")
datasets_available$results[,4] # titles
```

We'll work with the already available *Movielense* dataset.

``` {r}
data(MovieLense) # loads dataset
class(MovieLense)
movie_r <- MovieLense 
remove(MovieLense)
```

It is formatted as a `realRatingMatrix` class already, an object class created within `recommenderlab` for efficient storage of user-item ratings matrices. It's been optimized for storing sparse matrices, where almost all of the elements are empty. As an example, compare the object size of *Movielense* as a `realRatingMatrix` vs. a `matrix`. 

``` {r}
library(pryr)
object_size(movie_r)
object_size(as(movie_r, "matrix"))
```

The `realRatingMatrix` for this particular dataset is about 9 times more efficient in conserving memory than a traditional matrix object.

Some of the different functions that can be applied to the `realRatingMatrix` are: 

``` {r}
methods(class = "realRatingMatrix")
```

## Exploratory Analysis of the Movielense data

Some initial information about the dimensions and ratings count within Movielense matrix. 

``` {r}
movie_r
```

A preview of the first 10 users (rows of matrix) shows their count of movie ratings out of the 1664 available movies in the dataset. 

``` {r }
rowCounts(movie_r[1:10,])
```

Below is a preview of the ratings matrix of users and their ratings. Rows represent the user indexes.

``` {r }
getRatingMatrix(movie_r[1:10, 1:4])
```


For a particular user such as User 1, they gave an average rating of `r round(rowMeans(movie_r[1, ]), 2)`. 10 of the movies rated by them are shown below. 

``` {r eval = FALSE}
as(movie_r[1, ], "list")[[1]][1:10]
```

The `getRatings` function returns the non-missing ratings values from the matrix as a numeric vector. The following histogram shows the distribution of all movie ratings in the dataset. We can see that ratings typically skew higher, centered around a median rating of 4. 

``` {r echo = TRUE}
summary(getRatings(movie_r))

data.frame(ratings = getRatings(movie_r)) %>%
  ggplot(aes(ratings)) + geom_bar(width = 0.75) +
    labs(title = 'Movielense Ratings Distribution')

```

Using our `realRatingMatrix` object, we can also extract row counts to visualize distributions of the number of reviews given by each user. Below, the density is plotted along the y-axis instead of the raw counts, to give an idea of the the proportional frequency of each unit of each discrete bin in relation to the whole data set. The overall right-skewed distribution is indicative that most reviewers give very few overall reviews. 

In terms of understanding the density values, this histogram has bin-width set to 20; with a density of close to 0.01125 for the first bin, the tallest bar this bin represents approximately 0.01125 x 10 units per bin = `r 0.01125 * 20` total proportion of the individual reviewers in the data. In other words, `r 0.01125 * 20 * 100`% of the `r nrow(movie_r)` in the data have given fewer than 10 reviews. 

``` {r }
summary(rowCounts(movie_r))

rowCounts(movie_r) %>%
  data.frame(reviews_per_person = .) %>%
  ggplot(aes(x = reviews_per_person)) + 
    geom_histogram(aes(y = ..density..), binwidth = 20) +
    scale_y_continuous(limits = c(0,.0125), 
                       breaks = seq(0, .0125, by = 0.0025),
                       labels = seq(0, .0125, by = 0.0025)) +
    labs(title = 'Number of Ratings Per MovieLense Reviewer')

```

Additionally, we can take a look at the average number of ratings given per each of the `r ncol(movie_r)` movies. Again, the right-skewed distribution here is indicative that the majority of films in the dataset are scarcely reviewed and there are a handful of movies with very high reviews, probably reflecting those films in the dataset with mass commercial appeal. 

With a median number of reviews of `r median(colCounts(movie_r))` per user and `r ncol(movie_r)` different movies available to rate, we know that the data is sparse with a lot of users not having rated most of the movies available. 

``` {r }
summary(colCounts(movie_r))

colCounts(movie_r) %>%
  data.frame(movie_review_count = .) %>%
  ggplot(aes(x = movie_review_count)) + 
    geom_histogram(aes(y = ..density..), binwidth = 20) +
    scale_y_continuous(limits = c(0,.0175)) +
    labs(title = 'Number of Reviews Per MovieLense listed Movie')

```

**Can also visually explore summary(rowMeans(movie_r)) for average rating given per user.**

**Can also visually explore summary(colMeans(movie_r)) for average rating given per movie.**

## Recommender Algorithms Available

The recommender algorithms are stored in a registry object called `recommenderRegistry`. We can get a look at the different models based on the different matrix types.

``` {r}
names(recommenderRegistry$get_entries())
```

Since our matrix is a real ratings matrix, we'll call the algorithms available for working on numeric ratings based review data as stored in the `realRatingMatrix`. Here, I've pulled the descriptions of each of the algorithms available for working with real user ratings data. 

``` {r}
vapply(recommenderRegistry$get_entries(dataType = "realRatingMatrix"), 
       '[[', 
       FUN.VALUE = "character", 
       "description")
```

## Exploring User-based Collaborative Filtering

In the algorithms registry, the last algorithm provided in the listing is the one we'll use to explore user-based collaborative filtering (UBCF) to fit the UBCF algorithm to the `realRatingMatrix` of MovieLense reviews data. Information about this algorithm per the registry:

``` {r echo = TRUE}
ubcf_model_description <- tail(recommenderRegistry$get_entries(dataType = "realRatingMatrix"), 1)
ubcf_model_description
```

There are 4 parameters to account for with this model as described above:

* **method**: this is the type of similarity metric to calculate similarity between users real ratings profile. Cosine similarity, Pearson correlation coefficient, and Jaccard similarity are available options. The first two are not good options if using unary ratings, but work well for this scenario.

* **nn**: this parameter sets the neighborhood of most similar users to consider for each user profile. the ratings profiles of the k nearest neighbors will be the basis for making predictions on a users unrated items profile.

* **sample**: a logical value to indicate whether the data should be sampled for train/test. Probably best to explicitely set a reproducible seed and sample the data before running the model.

* **normalize**: how to normalize real ratings provided by different users. This is crucially important b/c all users have a different bias in how they tend to rate items. This can be done by passing a value to this parameter inside the algorithm or applied to the matrix before any modeling too. See `?normalize` for additional details. 

### Normalize the data 

Since we're working with explicit real ratings of users, we need to acocunt for individual row bias of each user and make sure that all ratings are scaled similarly. The implication of not doing this could be potentially disasterous on new predicted ratings for any given user, dependent upon the different ratings bias of their k nearest neighbors. 

User rating *zero mean centering* will be used for modeling, where each user's vector of ratings is subtracted by its own mean to center the mean at zero. Z-scoring is an alternative method available too that additionally divides each user's rating by its standard deviation. 

** maybe visualize the distribution of user ratings here too after normalization vs. before normalization **
``` {r}
movie_r_norm <- normalize(movie_r, method = 'center') # for visual comparison purposes. 
remove(movie_r_norm)
```

### How the UBCF algorithm works

1. Using [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity), figure out how similar each user is to each other. 
  i) for each user, identify the *k* most similar users. Here, *k* parameter was the 10 most similar users who rated common items most similarly. 
2. Per item, average the ratings by each user's *k* most similar users.
  i) weight the average ratings based on similarity score of each user whose rated the item. Similarity score equals weight, or
  ii) use any of the pythagorean averages, as suits the business case (arithmetic, geometric, harmonic)
3. Select a Top-N recommendations threshold. 


###  Set Up a Model Training & Evaluation Scheme

The next step is to set up a model training and testing scheme. There are many ways to go about doing this. The simplest is to build the recommender on a subset of training records, and test the model on a different subset of testing records that were withheld from the modeling process. We'll use the `evaluationScheme` function within `recommenderLab`.

``` {r echo = TRUE}
train_proportion <- .75
# shouldn't keep n rec. items > min(rowCounts(movie_r))
min(rowCounts(movie_r))
items_per_test_user_keep <- 10
# What's a good rating for a binary split?
good_threshold <- 4

```

The first thing to do is prepare the data, and set parameters for how the recommender algorithm will train the model. The scheme has been setup to use a single test dataset, train the data on a `r train_proportion * 100`% random sample of the data. In the test set,  
`r items_per_test_user_keep` items per user will be given to the recommender algorithm and the remaining test user's items will be held out for computing rating prediction error.

``` {r echo = TRUE}
# Building a Recommender System with R by Gorakala and Usuelli. Ch.4 pp 77 - 83
set.seed(123)
model_train_scheme <- evaluationScheme(data = movie_r,
                 method = 'split', # single train/test split
                 train = train_proportion, # proportion of rows to train. 
                 given = items_per_test_user_keep, # shouldn't keep n rec. items > min(rowCounts(movie_r))
                 goodRating = good_threshold, # for binary classifier analysis. 
                 k = 1)
```

Having set our `evaluationScheme` and stored it in an object called *model_train_scheme*, we can fit a UBCF recommender system model. 

``` {r echo = TRUE}
# Building a Recommender System with R by Gorakala and Usuelli. Ch.4 pp 84
model_params <- list(method = "cosine",
                     nn = 10, # find each user's 10 most similar users.
                     sample = FALSE, # already did this.
                     normalize = "center")

ubcf_model_train <- Recommender(data = getData(model_train_scheme, "train"), #only fit on the 75% training data.
                                method = "UBCF",
                                parameter = model_params)
```

### Evaluate Predictive Performance 

Having built the model, next step is to use the holdout testing data to evaluate the model's performance. The `getData` gives us access to different datasets in the model training scheme. We used the *train* data to build the model. There is also *known* and *unknown* test data available for evaluation. The *known* portion returns the specified `r items_per_test_user_keep` items per test user to run through the recommender algorithm. These *known* records per test user will calibrate the test user's similarity to the trained records, identify and weight its nearest *k* neighbors, and then make item ratings or recommendation predictions. The predicted ratings or recommended items are compared to the remaining items for each test user that are still hidden or *unknown*. These *unknown* data therefore will be used to help compute prediction error of the model. 

Since testing the algorithm with new data requires a known battery of item ratings to calibrate each test user and make recommendations on new items, and an unknown portion of ratings that can be used to calculate prediction error of these resulting recommendations, it's important that the *given* parameter is less than the minimum number of rated items available per user, so that *unknown* test data is available for every test case to measure prediction error of ratings. 

Next, we use the *known* part of the test users' item data (`r items_per_test_user_keep` items for each user) to make predicted ratings for new items of the test user that were hidden from the algorithm. We can also predict top N items instead of the ratings if that is preferred. 

``` {r echo = TRUE}
# 5.6. Evaluation of predicted ratings in recommenderLab vignette. 
# https://cran.r-project.org/web/packages/recommenderlab/vignettes/recommenderlab.pdf
ubcf_model_test_pred <- predict(ubcf_model_train, getData(model_train_scheme, "known"), type = "ratings")

```



## Strengs & Weaknesses of Neighborhood Methods

* *Data Requirements*: a user ratings profile, containing items theyâ€™ve rated/clicked/purchased. A "rating" can be defined however it fits the business use case.
        	
* *Strengths*: simple to implement, and recommendations are easy to explain to user. Transparency about the recommendation to a user can be a great boost to the user's confidence in trusting a rating. 
 
* *Weaknesses*: these algorithms do not too work well on very sparse ratings matrices. Additionally, they are computationally expensive as the entire user database needs to be processed as the basis of forming recommendations. These algorithms will not work from a cold start since a new user has no historic data profile or ratings for the algorithm to start from. 


``` {r eval = FALSE, include = FALSE}

#So we can more objectively measure the performance of the UBCF recommender system, we will build it on a subset of training #records, and test the model on a different subset of testing records that were withheld from the modeling process. 

#set.seed(123)
#train_index <- sample(c(TRUE, FALSE), size = nrow(movie_r), 
#                      replace = TRUE, 
#                      prob = c (.7, .3)) # weights for train/test size

#train <- movie_r[train_index, ]
#test <- movie_r[!train_index, ]

# check that train and test records sum up to movie_r records count. 
#sum(nrow(train), nrow(test)) == nrow(movie_r)

# Having zero mean centered each user's ratings to control for user bias, and splitting the data into model training and model testing partitions, time to fit a model. Parameters were tuned from defaults for illustrative purposes. 
#rec_model_train <- Recommender(train, method = "UBCF", 
#                          parameter = list(method = "cosine",
#                                           nn = 10, # find each user's 10 most similar users. 
#                                           sample = FALSE, # already did this.
#                                           normalize = "center")) # already did this. 

#rec_model_train_details <- getModel(rec_model_train)
# the attributes from the getModel() function.
#names(rec_model_train_details)
#rec_model_train_details$data

```
